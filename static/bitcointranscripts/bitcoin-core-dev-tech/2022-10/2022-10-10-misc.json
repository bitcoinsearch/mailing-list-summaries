{
    "id": "bitcointranscripts+bitcoin-core-dev-tech+2022-10+2022-10-10-misc",
    "title": "Misc",
    "body_formatted": "{\"type\":\"heading\",\"text\":\"Web of Trust\"}, {\"type\":\"paragraph\",\"text\":\"Some of the public key server operators interpreted GDPR to mean that they can't operate public key infrastructure anymore. There needs to be another solution for p2p distribution of keys and Web-of-Trust.\"}, {\"type\":\"paragraph\",\"text\":\"<bitcoin-otc.com> continues to be the longest operating PGP web-of-trust using public key infrastructure. Rumplepay might be able to bootstrap a web-of-trust over time.\"}, {\"type\":\"heading\",\"text\":\"Stealth addresses and silent payments\"}, {\"type\":\"paragraph\",\"text\":\"Here's something controversial. Say you keep an in-memory map of all addresses that have already been used. Those are definitely not for you if they are reused addresses, because you don't do that. And that's most of the addresses. This will save you on which transactions you look at; it can't be your output. This also creates an incentive to convince other people to reuse addresses because it reduces your scanning costs. You can make a bloom filter of all the reused addresses and check that. Well, you can have false positives, so that screws the bloom filter over. It's like a scarlet letter list. Here's all the bad people, right? If you implement it correctly, the addresses are provably unique.\"}, {\"type\":\"paragraph\",\"text\":\"The real zero-conf should be \\\"just send the private key\\\". That's even faster than 0conf. Wonder why that wasn't the schelling point that people organized around.\"}, {\"type\":\"heading\",\"text\":\"Anonymous traffic networks\"}, {\"type\":\"paragraph\",\"text\":\"Packet anonymizer using an HSM: the privacy of the traffic is protected by the HSM root of trust. Use an HSM to do tor traffic routing. This could be architected in a way that makes it easier to detect if there is someone tampering with a network, or create an incentive or pressure to make more strongly protected roots of trust, like decentralized roots of trust covenanted by groups.\"}, {\"type\":\"heading\",\"text\":\"Signing by hand\"}, {\"type\":\"paragraph\",\"text\":\"One idea is to encrypt/decrypt by hand and export the signing process to a computer using fully homomorphic encryption. You could replace the signature algorithm with a fully homomorphic version of the signature algorithm. Or if it's mostly lattice calculations, then it may be possible to do it by hand. Say you get a fully homomorphically encrypted blob and then there's only one algorithm that can run on it to spit out 0 or 1 for verification.\"}, {\"type\":\"paragraph\",\"text\":\"Maybe if you do the thing where you have a fully homomorphically encrypted program, and then the program can't be efficiently modified by other people? Do you need a zero-knowledge proof of signature verification? Or would that need to be run by hand? You need a proof of correct execution which is hard to avoid.\"}, {\"type\":\"paragraph\",\"text\":\"Could you find a curve that would be easier to do by hand? Maybe a binary curve over a binary field. The base field instead of being a large prime would be a Galois field with 2^256 elements or something. I don't know if the operations are easier to do there. I would assume humans might find it easier to operate on base 10. The existing stuff that Andrew has works over the field of 32 elements. But then there's extra machinery to work with...\"}, {\"type\":\"paragraph\",\"text\":\"I suggested a long time ago, no idea how feasible it is, to use mechanical aides. There are computers that can evaluate Fourier transforms. There are multiplication algorithms that are based on Fourier transforms. Like bead machines or mechanical hardware. Something like that maybe could be made to work. It's a matter of time. To evaluate one scalar multiplication of a curvepoint involves something like a thousand Fourier approximations... each point is 10, so there's 100 of them, so there's about 1,000.\"}, {\"type\":\"paragraph\",\"text\":\"We were discussing this yesterday. You could pre-compute... you could have computation look-up tables for all powers already written down in a book and maybe you could do things more efficiently that way. It seems like it would be easy to put in small errors that would result in wrong values that would leak information.\"}, {\"type\":\"paragraph\",\"text\":\"The best case would be like one pairing check. How hard is it to do a pairing computation by hand? It's much more expensive than doing just a single point addition, but maybe it's better than a point multiplication? The expensive part of computing a pairing is the pairing for like BLS-12, say, goes into the field of size p^12 so then there's some r value that is about 256 bits that divides p^12 - 1. So you have to do a multiplication of p^12 - 1 divided by r which is a value with approximately 2000 bits or something. It's a big exponentiation.\"}, {\"type\":\"paragraph\",\"text\":\"Bulletproofs verification by hand would be like Sysphus pushing up a bolder by hand. Doing 1000 point multiplications by hand? Make one tiny error and it's all over. If you make one small tiny error, do you still get a point on the curve? If you make a small error, it depends on how you do it. A random error? If you pick a random x, there's a 50% chance that it's a curvepoint. But a random x and y that you pick? It's most likely not a curvepoint.\"}, {\"type\":\"paragraph\",\"text\":\"Something that you could compute by hand that would make it easier to verify zero-knowledge proofs by hand would be likely to be useful for improving zero-knowledge verifications on computers since it would be easier.\"}",
    "body": "\n# Web of Trust\n\nSome of the public key server operators interpreted GDPR to mean that they can't operate public key infrastructure anymore. There needs to be another solution for p2p distribution of keys and Web-of-Trust.\n\n<bitcoin-otc.com> continues to be the longest operating PGP web-of-trust using public key infrastructure. Rumplepay might be able to bootstrap a web-of-trust over time.\n\n# Stealth addresses and silent payments\n\nHere's something controversial. Say you keep an in-memory map of all addresses that have already been used. Those are definitely not for you if they are reused addresses, because you don't do that. And that's most of the addresses. This will save you on which transactions you look at; it can't be your output. This also creates an incentive to convince other people to reuse addresses because it reduces your scanning costs. You can make a bloom filter of all the reused addresses and check that. Well, you can have false positives, so that screws the bloom filter over. It's like a scarlet letter list. Here's all the bad people, right? If you implement it correctly, the addresses are provably unique.\n\nThe real zero-conf should be \"just send the private key\". That's even faster than 0conf. Wonder why that wasn't the schelling point that people organized around.\n\n# Anonymous traffic networks\n\nPacket anonymizer using an HSM: the privacy of the traffic is protected by the HSM root of trust. Use an HSM to do tor traffic routing. This could be architected in a way that makes it easier to detect if there is someone tampering with a network, or create an incentive or pressure to make more strongly protected roots of trust, like decentralized roots of trust covenanted by groups.\n\n# Signing by hand\n\nOne idea is to encrypt/decrypt by hand and export the signing process to a computer using fully homomorphic encryption. You could replace the signature algorithm with a fully homomorphic version of the signature algorithm. Or if it's mostly lattice calculations, then it may be possible to do it by hand. Say you get a fully homomorphically encrypted blob and then there's only one algorithm that can run on it to spit out 0 or 1 for verification.\n\nMaybe if you do the thing where you have a fully homomorphically encrypted program, and then the program can't be efficiently modified by other people? Do you need a zero-knowledge proof of signature verification? Or would that need to be run by hand? You need a proof of correct execution which is hard to avoid.\n\nCould you find a curve that would be easier to do by hand? Maybe a binary curve over a binary field. The base field instead of being a large prime would be a Galois field with 2^256 elements or something. I don't know if the operations are easier to do there. I would assume humans might find it easier to operate on base 10. The existing stuff that Andrew has works over the field of 32 elements. But then there's extra machinery to work with...\n\nI suggested a long time ago, no idea how feasible it is, to use mechanical aides. There are computers that can evaluate Fourier transforms. There are multiplication algorithms that are based on Fourier transforms. Like bead machines or mechanical hardware. Something like that maybe could be made to work. It's a matter of time. To evaluate one scalar multiplication of a curvepoint involves something like a thousand Fourier approximations... each point is 10, so there's 100 of them, so there's about 1,000.\n\nWe were discussing this yesterday. You could pre-compute... you could have computation look-up tables for all powers already written down in a book and maybe you could do things more efficiently that way. It seems like it would be easy to put in small errors that would result in wrong values that would leak information.\n\nThe best case would be like one pairing check. How hard is it to do a pairing computation by hand? It's much more expensive than doing just a single point addition, but maybe it's better than a point multiplication? The expensive part of computing a pairing is the pairing for like BLS-12, say, goes into the field of size p^12 so then there's some r value that is about 256 bits that divides p^12 - 1. So you have to do a multiplication of p^12 - 1 divided by r which is a value with approximately 2000 bits or something. It's a big exponentiation.\n\nBulletproofs verification by hand would be like Sysphus pushing up a bolder by hand. Doing 1000 point multiplications by hand? Make one tiny error and it's all over. If you make one small tiny error, do you still get a point on the curve? If you make a small error, it depends on how you do it. A random error? If you pick a random x, there's a 50% chance that it's a curvepoint. But a random x and y that you pick? It's most likely not a curvepoint.\n\nSomething that you could compute by hand that would make it easier to verify zero-knowledge proofs by hand would be likely to be useful for improving zero-knowledge verifications on computers since it would be easier.\n\n",
    "body_type": "markdown",
    "created_at": "2022-10-10T00:00:00.000Z",
    "domain": "https://btctranscripts.com/",
    "url": "https://btctranscripts.com/bitcoin-core-dev-tech/2022-10/2022-10-10-misc",
    "categories": [
        "core-dev-tech"
    ],
    "tags": [
        "p2p",
        "bitcoin core"
    ],
    "indexed_at": "2024-03-21T16:33:35.991Z",
    "transcript_by": "Bryan Bishop",
    "summary": "In the realm of digital security and blockchain, evolving technologies and regulatory landscapes present both challenges and innovative solutions. The interpretation of the General Data Protection Regulation (GDPR) by some public key server operators as a barrier to the continuation of their services underscores the pressing need for alternative methods in the distribution of cryptographic keys. This has propelled platforms like <bitcoin-otc.com> into significance as it remains one of the longest-operating PGP web-of-trust utilizing public key infrastructure. Rumplepay emerges as a potential candidate to gradually establish a similar web-of-trust structure, indicating a shift towards peer-to-peer (P2P) distribution mechanisms as viable responses to regulatory constraints.\n\nThe concept of stealth addresses and silent payments introduces a controversial yet clever approach to enhancing transaction privacy within blockchain networks. By maintaining an in-memory map of reused addresses and excluding them from scrutiny, this method inherently encourages address uniqueness, thus reducing unnecessary transaction examination. Though this strategy might incentivize address reuse among others to decrease scanning costs for oneself, implementing bloom filters could mitigate such concerns despite the possibility of false positives. This system aims at fostering provable uniqueness in address generation, thereby streamlining transaction privacy without compromising on efficiency.\n\nExploring the domain of anonymous traffic networks unveils the potential of leveraging Hardware Security Modules (HSM) for packet anonymization. Utilizing HSMs for Tor traffic routing not only strengthens the privacy of data in transit but also facilitates the detection of network tampering. This proposition advocates for the creation of robust roots of trust, possibly decentralized, endorsed by collective covenants, aiming to enhance the integrity and security of data transmission across networks.\n\nDelving into the intricacies of cryptographic verification, the dialogue shifts towards the feasibility of manual encryption/decryption processes augmented by fully homomorphic encryption technologies. This technique contemplates replacing traditional signature algorithms with homomorphic counterparts or considering lattice calculations that could potentially be executed manually. While this idea challenges conventional practices, it opens avenues for exploring encryption through more straightforward, perhaps even mechanical, means. The discussion further entertains the notion of employing mechanical aids like bead machines for computational tasks, hinting at the blend of historical computation techniques with modern cryptographic needs.\n\nLastly, the conversation navigates the complexities of manually verifying cryptographic proofs, specifically addressing bulletproofs and pairings. Despite the theoretical possibility of hand-computing certain cryptographic operations, the practicality of executing complex calculations like pairing computations or bulletproof verifications is daunting. The risk of introducing errors, which could significantly compromise security or result in incorrect outcomes, casts doubt on the viability of manual calculations for intricate cryptographic protocols. This segment emphasizes the inherent challenges in adapting sophisticated cryptographic principles to manual execution, suggesting a need for innovative simplification or automation to ensure accuracy and efficacy in cryptographic verifications.\n\nThrough these discussions, it becomes evident that the intersection of regulatory compliance, privacy enhancement, and cryptographic innovation calls for a delicate balance between practicality and security. As the digital landscape evolves, so too must the approaches to ensuring the privacy, integrity, and security of digital transactions and communications."
}