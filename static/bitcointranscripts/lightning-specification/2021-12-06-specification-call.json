{
    "id": "bitcointranscripts+lightning-specification+2021-12-06-specification-call",
    "title": "Lightning Specification Meeting - Agenda 0943",
    "body_formatted": "{\"type\":\"paragraph\",\"text\":\"Name: Lightning specification call\"}, {\"type\":\"paragraph\",\"text\":\"Topic: Agenda below\"}, {\"type\":\"paragraph\",\"text\":\"Location: Google Meet\"}, {\"type\":\"paragraph\",\"text\":\"Video: No video posted online\"}, {\"type\":\"paragraph\",\"text\":\"Agenda: <https://github.com/lightning/bolts/issues/943>\"}, {\"type\":\"paragraph\",\"text\":\"The conversation has been anonymized by default to protect the identities of the participants. Those who have given permission for their comments to be attributed are attributed. If you were a participant and would like your comments to be attributed please get in touch.\"}, {\"type\":\"heading\",\"text\":\"Add payment metadata to payment request\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/pull/912>\"}, {\"type\":\"paragraph\",\"text\":\"In theory you could have an invoice that is so large that it forces you to use a direct payment. But that is a pretty bad theoretical case. We could make some handwavey comment that users should be aware that it has to go in the onion so it should be of limited length. But that is like saying \u201cDon\u2019t do anything stupid\u201d.\"}, {\"type\":\"paragraph\",\"text\":\"I asked on the issue before the meeting, is there any concern with someone who is making a repeated payment who tries to detect the length of the route in use by varying the payment data length over time across various retries?\"}, {\"type\":\"paragraph\",\"text\":\"How would they be able to do that, because of the payment secret thing?\"}, {\"type\":\"paragraph\",\"text\":\"As long as you know when a payment failed. If you have the ability to send someone ten invoices and have them try them in order and detect which ones fail then you would have the ability to figure out the length of the path they are using, at least currently.\"}, {\"type\":\"paragraph\",\"text\":\"You mean the recipient is de-anonymizing the sender? Not an intermediate node?\"}, {\"type\":\"paragraph\",\"text\":\"Basically, yes.\"}, {\"type\":\"paragraph\",\"text\":\"Maybe an intermediate option is to limit it to a large number just to prevent that.\"}, {\"type\":\"paragraph\",\"text\":\"That was basically what I was suggesting, limit it to something. I don\u2019t know if you need 700, limit it to 256 or something.\"}, {\"type\":\"paragraph\",\"text\":\"640 is traditional.\"}, {\"type\":\"list\"}, {\"type\":\"paragraph\",\"text\":\"I would make it longer than 28 because that seems really small. There might be some applications that are excluded by that. I think it is already quite exotic what you described. You are able to supply multiple invoices that are all attempted and you are going to derive something out of it about the distance. I would pick a large value.\"}, {\"type\":\"paragraph\",\"text\":\"Note that any amount we limit it to is basically plus 32 because you can always use the payment secret as well.\"}, {\"type\":\"paragraph\",\"text\":\"There are other fields that are always there. What\u2019s the minimum for a direct payment?\"}, {\"type\":\"paragraph\",\"text\":\"That attack already exists because I can hand you a fake route hint of arbitrary length that you have to use to make the payment, therefore leaving less room in the onion.\"}, {\"type\":\"paragraph\",\"text\":\"This is true.\"}, {\"type\":\"paragraph\",\"text\":\"Shall I add a comment to the PR that you have to be aware that it needs to fit in the onion including the route?\"}, {\"type\":\"paragraph\",\"text\":\"Yeah.\"}, {\"type\":\"paragraph\",\"text\":\"I did also comment on the PR. I think if we do that then we should at least also say that a payer must not limit the length except when they create the onion ie if someone knows they are well connected node they can just use a larger value. The payer must not apply any concrete limits.\"}, {\"type\":\"paragraph\",\"text\":\"To prevent the opposite thing, people limiting more than necessary.\"}, {\"type\":\"paragraph\",\"text\":\"Yes, exactly.\"}, {\"type\":\"paragraph\",\"text\":\"Let\u2019s add that, let\u2019s implement it and let\u2019s do compatibility testing again.\"}, {\"type\":\"heading\",\"text\":\"Advertize compression algorithms in init\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/pull/825>\"}, {\"type\":\"paragraph\",\"text\":\"Let\u2019s move onto the next one, the compression algorithm support thing. It is an old one, it was removing the dependency on zlib for implementations that did not want to support it. At the same time make it flexible enough to add new encodings or new compression algorithms if we ever need to. Make sure that this is also something that is more general than gossip queries if we ever need to. We can do it for free even though I\u2019m sure we\u2019ll ever use it. Basically you send a bitfield in a TLV in your init saying \u201cHere are the compression algorithms I support\u201d. People for gossip queries only use the things that both you and I support. We have tested with Matt on testnet and it worked great with LDK and eclair. It is not very hard to implement.\"}, {\"type\":\"paragraph\",\"text\":\"It is so much easier to implement zlib than it is to actually implement this.\"}, {\"type\":\"paragraph\",\"text\":\"It adds a bit of complexity. It is still one feature. It is not very hard.\"}, {\"type\":\"paragraph\",\"text\":\"I think what happens in the long term, we will end up using Minisketch for gossip I\u2019m pretty sure. This becomes less important. I am happy to take your advice. If having implemented it if you are happy with the complexity of it sure.\"}, {\"type\":\"paragraph\",\"text\":\"I find it simple but it provides extension points that I think we don\u2019t really need. But maybe we never use them so it is fine. I agree that the next changes we will make to gossip queries or to gossiping overall will be mostly a complete rewrite using something like Minisketch. We will change everything so we will throw this away. But I can understand that if LDK needs something to start with\u2026\"}, {\"type\":\"paragraph\",\"text\":\"There are a lot of nodes on the network that already use non-zlib. In general it isn\u2019t really an issue if you connect to enough nodes to get one that will send you uncompressed data. Given that I am not sure which nodes it is. Does lnd send uncompressed data by default or something?\"}, {\"type\":\"paragraph\",\"text\":\"It did for a while. We do a heuristic where we look at the compressed length and we go \u201cIt is not worth compressing\u201d which is probably a waste of time. We\u2019ve compressed it already, we should probably just use it. We do go \u201cThat didn\u2019t win so we won\u2019t bother\u201d. If you ask for enough you\u2019ll get compressed data.\"}, {\"type\":\"paragraph\",\"text\":\"It seems like less complexity than that. My point there was that it seems there are a number of nodes on the network that need this basically.\"}, {\"type\":\"paragraph\",\"text\":\"I think everyone has implemented decompression, I think some people haven\u2019t implemented compression so that is what you are seeing. Implementing decompression is the easier part. The compression involves heuristics, you\u2019ve got to fit it in a message and you\u2019re like \u201cI don\u2019t know how much it is going to compress until I compress it\u201d and stuff like that.\"}, {\"type\":\"paragraph\",\"text\":\"Interestingly we do already have these hooks. We already have the compression flag stuff. At least on the LDK end I just reuse those enums for compression because they happen to line up, at least currently.\"}, {\"type\":\"paragraph\",\"text\":\"Basically I think the state of this PR is we know we can do it, it is not too hard but do we really want to do it? Do we need it?\"}, {\"type\":\"paragraph\",\"text\":\"It is going to be quite a while before we rewrite stuff and not everyone wants the zlib dependency because zlib has not had the best history of ensuring its decompression library is safe.\"}, {\"type\":\"paragraph\",\"text\":\"It is just you, everyone else is happy with it, everyone else is stuck with it is the truth. zlib has a bad name but those are in the past. It has been pretty thoroughly vetted now. zlib is a lot smaller. The main issue with zlib was the whole explosive compression thing where you could really cram a lot of data in if people made assumptions about how much they could get out. Which is why the spec went through and figured out the maximum realistic size. If you get more than that you just throw it away. I don\u2019t know. Shying away from zlib because it has bugs, if you can\u2019t trust zlib who can you trust?\"}, {\"type\":\"paragraph\",\"text\":\"There is just not enough data in the network graph right now to care and so it is very easy to say \u201cWe can avoid bugs by simply not compressing because we don\u2019t care and eventually we will move to something that is substantially more proficient than any of this anyway\u201d.\"}, {\"type\":\"paragraph\",\"text\":\"I do buy that argument.\"}, {\"type\":\"paragraph\",\"text\":\"But then if we do that it is maybe not worth integrating this PR and instead phasing out zlib.\"}, {\"type\":\"paragraph\",\"text\":\"You mean drop zlib entirely? We could do that. I have a feeling that is not going to fly mostly with y\u2019all on the mobile side. I know mobile using this stuff is important assuming you are doing graph sync on mobile.\"}, {\"type\":\"paragraph\",\"text\":\"We don\u2019t care for Phoenix because there\u2019s no graph sync. I don\u2019t think eclair mobile even has the zlib part. Maybe but I\u2019m not sure. I will ask Breez to see if they care and if they extensively use zlib.\"}, {\"type\":\"paragraph\",\"text\":\"Going back to first principles I\u2019d be quite happy to just drop zlib. I would want to check some numbers but that is simple. I could do that commit. And it is perfectly backwards compatible. Stop compressing, keep decompressing and then at some point turn off decompression.\"}, {\"type\":\"paragraph\",\"text\":\"I\u2019ll try to get in touch with a few wallets. See if they are using it, if they even know if they are using it or not. And if they can measure the difference between the two and see if they really need it. Maybe they\u2019ll say that the fact that it uses more battery is more of an issue for them than the fact that it uses more bandwidth. I don\u2019t even know if it is something that is important for them.\"}, {\"type\":\"paragraph\",\"text\":\"You are just not going to download the graph fast enough on mobile to want to do that anyway. At least not from a peer-to-peer node.\"}, {\"type\":\"paragraph\",\"text\":\"Eugene is saying that lnd doesn\u2019t even have a config option for zlib by default. Unless Breez forked it they couldn\u2019t be using zlib at all. I\u2019ll check with the Breez team and if they are not using it I guess we can go and start deprecating it. At least open a PR and tell the mailing list and see if anyone says that they absolutely want to keep it.\"}, {\"type\":\"heading\",\"text\":\"Dynamic DNS support in gossip messages\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/pull/911>\\n<https://github.com/lightning/bolts/pull/917>\"}, {\"type\":\"paragraph\",\"text\":\"I guess the second one we don\u2019t even need to talk about it because nothing has changed since last time. There is an eclair implementation and there is an ongoing c-lightning one but it is not ready yet. But the DNS one I haven\u2019t done it on eclair at all. I don\u2019t know if anyone else has worked on it?\"}, {\"type\":\"paragraph\",\"text\":\"911 we have merged at least as an experimental option. DNS support we have. We can put DNS fields in advertizements, it seems to work pretty much as you\u2019d expect. It is a fairly straightforward change. 911 we have implemented, we should probably get someone else to implement it as well. There is a draft PR for 917, the init message change. Without Michael (Schmoock) here I don\u2019t know what the status is but I can look at it.\"}, {\"type\":\"paragraph\",\"text\":\"I think it is not filtering local addresses yet. But apart from that he has the TLV and I tested that the TLV decodes fine on the eclair side. But he doesn\u2019t yet filter local addresses.\"}, {\"type\":\"paragraph\",\"text\":\"That\u2019s weird because we have that code already. That\u2019s how we decide whether or not by default we advertize our own address. He probably just hasn\u2019t hooked it up.\"}, {\"type\":\"paragraph\",\"text\":\"I think we can discuss these two when there is momentum in the implementations.\"}, {\"type\":\"paragraph\",\"text\":\"They are completely independent. I guess it would be nice if somebody implemented DNS, at least the lookup side.\"}, {\"type\":\"paragraph\",\"text\":\"One question on DNS. Punycode or UTF-8? It should be specified.\"}, {\"type\":\"paragraph\",\"text\":\"Interestingly in the offers spec we add a UTF-8 type to the fundamental types which is basically a byte string. It gives you a nice hint that something should be in an array of UTF-8. It might be useful to pull that in. It should be specified.\"}, {\"type\":\"paragraph\",\"text\":\"I vaguely prefer that this be ASCII and Punycode, DNS anyway. Things that do DNS tend to do the reverse resolution fairly well. I\u2019m ok being overruled. It is just easier to walk a string and check that it is ASCII than it is to walk a string and check that it is UTF-8 with no control characters.\"}, {\"type\":\"paragraph\",\"text\":\"What\u2019s the worst that can happen with UTF-8? It is ourself announcing and as far as I can see the only thing that we could get tricked into is trying to resolve this kind of stuff. It is not like the human is going to read it and get tricked by characters that look alike.\"}, {\"type\":\"paragraph\",\"text\":\"Ultimately when you do the resolution you convert it to ASCII and Punycode anyway. If you are looking to do that you might as well just do that upfront. Most of the DNS applications that I\u2019ve seen will do that reverse resolution for you if you want it.\"}, {\"type\":\"paragraph\",\"text\":\"Is that true?\"}, {\"type\":\"paragraph\",\"text\":\"I know at least in some web browsers if you type in the Punycode it will show it to you as the UTF-8 as long as the TLD is one of the internationalized ones. At least this used to be true.\"}, {\"type\":\"paragraph\",\"text\":\"I think there are some domains you won\u2019t be able to reach if you can\u2019t do Unicode but I could be wrong on that.\"}, {\"type\":\"paragraph\",\"text\":\"No everything gets converted to ASCII. You take the UTF-8, you convert it to ASCII with this weird prefix, accent dash dash or something like that, I forget what the actual prefix is. You encode the UTF-8 as ASCII and then you do the resolution.\"}, {\"type\":\"paragraph\",\"text\":\"I am not a Punycode expert but that means that Punycode by definition is at least as long as UTF-8 right? Would that be an argument? It is gossip data that has to be replicated on each individual node so any byte we can save there might be a massive win for us. If we have to have some sort of tiebreaker.\"}, {\"type\":\"paragraph\",\"text\":\"It is only an extra byte or two. Obviously I prefer ASCII for everything because it is easier than trying to think about attack scenarios but if someone feels very strongly about byte length I\u2019m ok with that too.\"}, {\"type\":\"paragraph\",\"text\":\"I guess I would prefer ASCII as well because it is just simpler. People can just find an ASCII hostname, there is no good reason to have something that is not ASCII for Lightning nodes.\"}, {\"type\":\"paragraph\",\"text\":\"If people insist on having weird hostnames they will shoot themselves in the foot by them paying the cost to advertize that. Probably going to stick with ASCII myself.\"}, {\"type\":\"paragraph\",\"text\":\"Everyone pays the price because it is gossiped. To be honest I just looked up Punycode, I had completely forgotten that existed. I am a little bit horrified. I remember before UTF-8 ruled the world. A certain amount of PTSD. I would say yes, it is ASCII due to DNS limitations, that is fine.\"}, {\"type\":\"paragraph\",\"text\":\"Alright, I will comment on the PR.\"}, {\"type\":\"paragraph\",\"text\":\"And make sure you put a link to the Wikipedia Punycode article so people can share the horror if they haven\u2019t been exposed before.\"}, {\"type\":\"paragraph\",\"text\":\"And encode the link in Punycode itself.\"}, {\"type\":\"paragraph\",\"text\":\"Yeah, do that.\"}, {\"type\":\"paragraph\",\"text\":\"To one of internationalized versions.\"}, {\"type\":\"heading\",\"text\":\"BLIPs\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/blips>\\n<https://github.com/lightning/blips/pull/4>\"}, {\"type\":\"paragraph\",\"text\":\"I am ambivalent about whether they should go in together or go in separate. Whichever allows us to get this stuff merged before the year ends is what I\u2019m in favor of.\"}, {\"type\":\"paragraph\",\"text\":\"Maybe separate is a good idea. The one I opened will be updated very often. Every time someone adds a BLIP they will have to open this table of things that they want to reserve. Whereas they should not open things about the meta process of adding a BLIP.\"}, {\"type\":\"paragraph\",\"text\":\"Agreed, I think that makes sense.\"}, {\"type\":\"paragraph\",\"text\":\"Keeping the two separate I think makes sense. I haven\u2019t read the latest version of BLIP 1 since it has moved to the new repo. I need to do that right now.\"}, {\"type\":\"paragraph\",\"text\":\"I listed in the initial commit the changes that I made. One was per Matt\u2019s feedback out of this specific mandatory universality section requesting the proposals discuss why the given features are not intended to be universal. That\u2019s a must now. Add a little bit more detail around what ranges of feature bits and TLVs belong here. Between 100 and 1000 for experimentation on feature bits and then TLVs above 65, 536. Then a couple of links and stuff. Otherwise I think it is largely the same that we were close to having consensus on in the previous repo.\"}, {\"type\":\"paragraph\",\"text\":\"I guess it sounds like a good start to me. I think we will probably change some of those as we go, as we learn how people write BLIPs and what are the pain points. I think we should start with something as small and simple as we can. And then make it evolve as we learn more about the process. That looks good enough.\"}, {\"type\":\"paragraph\",\"text\":\"This looks good to me. I\u2019m sure we are going to have some level of friction over the status field. The BIP process uses them for a little bit of pseudo inside baseball sometimes. I have a feeling that we are going to want to revisit these as we go but that is ok. I don\u2019t think we need to figure that out now.\"}, {\"type\":\"paragraph\",\"text\":\"I think in general having this as a starting point and continually iterating on it as people start using the process makes a lot of sense.\"}, {\"type\":\"paragraph\",\"text\":\"I agree. I will do a real review this week. It is a conceptual ACK from me at least.\"}, {\"type\":\"paragraph\",\"text\":\"I will do the same for yours. For PR 4.\"}, {\"type\":\"heading\",\"text\":\"Route blinding\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/pull/765>\"}, {\"type\":\"paragraph\",\"text\":\"There has been a new review on route blinding. There has been a complete compatibility test between c-lightning and eclair on the latest version of onion messages. Did anyone have time to look at the route blinding one? My main feedback from the last meeting, I was asking what parts I should remove to get a first version in. Mostly probably the parts about the payments and I should keep only the parts that are library cryptographic utilities that are being used by onion messages. Should I keep it in the proposal document where it is not specified yet and we can iterate on it? I updated the test vectors as well and I think they should be easier for you to work with and build upon for onion messages.\"}, {\"type\":\"paragraph\",\"text\":\"Great. We have had route blinding for payments for well over a year as an experimental option. Basically unused except for the test code because there was no good way to specify it. When we tried to revise it to the latest thing we hit those issues that I commented on the PR. But it works really well for the onion message as a base and we have interop testing so I am tempted to leave it there in the proposal document but just not in the spec.\"}, {\"type\":\"paragraph\",\"text\":\"That sounds good to me. I didn\u2019t put anything in the BOLTs related to messages, I left it in the proposal. I think the only important parts to discuss are the potential unblinding attacks that you would do by probing fees, probing CLTV expiry delta. It is basically things that we recommend implementations do like use the same CLTV expiry delta across the whole route. Use something that is different from the public values but maybe higher, same for the fees. I am not sure how to best convey those in the spec.\"}, {\"type\":\"paragraph\",\"text\":\"While I can specify that you should use the same value across you can always ignore those values and choose to probe the differences. The ultimate answer is you put it in the encrypted TLV, you put \u201cBy the way please enforce this value\u201d so they can\u2019t play with it.\"}, {\"type\":\"paragraph\",\"text\":\"That makes it bigger.\"}, {\"type\":\"paragraph\",\"text\":\"Perfect is the enemy of the good, yeah. That is another thing that can potentially be added later. I think for now a graph is relatively homogenous anyway so you wouldn\u2019t get all that much data. Although you would get some doing those games. We have a way of fixing it later if we want.\"}, {\"type\":\"paragraph\",\"text\":\"What would you tell an intermediate node that is inside the blinded route when they receive a payment that they should forward and the fee is not enough or the CLTV is not enough? What error should they answer?\"}, {\"type\":\"paragraph\",\"text\":\"They have to fail. We have an error that we return from anywhere in the blinded tunnel, in that TLV we return the same error. In fact our implementation, if it was blinded we always reply with the same error. You\u2019ll actually see it from the entrance to that TLV scheme and you\u2019ll never see an error from the middle. That\u2019s partially to protect against this attack. Although it is not perfect because you can still do success or failure tests. You get one bit of information out but you can\u2019t tell exactly where it comes from. I\u2019ll have to look up the code to see what we do but we have a specific error, I think we added an error for it. It has been a while, I\u2019ll have to look.\"}, {\"type\":\"paragraph\",\"text\":\"From a honest sender\u2019s point of view if you get such an error that tells you something wrong happened inside the blinded route, maybe it is fees, maybe it is CLTV expiry delta, should you just retry by raising everything for every blinded hop so that you have a chance of success? I think you should because there is a chance that your payment will go through. This is why it makes it better compared to rendezvous, you can retry, still use that blinded route and adapt to dynamic fees changing.\"}, {\"type\":\"paragraph\",\"text\":\"That is really hard to know. Your chances of success at that stage have surely dropped significantly. It is really up to the person handing you the route to have done that padding if any for you. If they haven\u2019t maybe you could retry but you don\u2019t actually know what is going on. It could be that there\u2019s temporary disconnect in the route, it is not working anymore. That\u2019s possibly the more likely case. It just won\u2019t work. In which case you are kind of out of luck at that point. Unless they hand you multiple encrypted routes which is allowed in the spec but we haven\u2019t implemented it.\"}, {\"type\":\"paragraph\",\"text\":\"I think that makes sense from the recipient\u2019s point of view. Especially if you have multiple entry points. Just give most of them in the invoice. The idea is the same way you do route hints, you\u2019d now use these. You can hand out multiple. The case where you don\u2019t allow it\u2026 For payments it is a little bit different. For onion messages you only give a single reply path but the idea is you will resend if it doesn\u2019t work and maybe try a different reply path.\"}, {\"type\":\"paragraph\",\"text\":\"Even from a payment\u2019s point of view as a recipient if you are not well connected at all, you just have one entry point, since it is blinded you can still make it look like you have many entry points and provide many fake route hints. I think it is a good idea to have this option.\"}, {\"type\":\"paragraph\",\"text\":\"I agree. That\u2019s for payments. We\u2019d need to test that. But for onion messages this route blinding works really well.\"}, {\"type\":\"paragraph\",\"text\":\"I saw that Tim made a comment about typo. Do you know if he has some time available to review the crypto? I discussed it with Jonas Nick in El Salvador. He said it could be on his list because he didn\u2019t realize it was a requirement for offers. They have so much to do already, I\u2019m not sure if they will find the time to take a look at it.\"}, {\"type\":\"paragraph\",\"text\":\"I will beg and see what happens.\"}, {\"type\":\"paragraph\",\"text\":\"Sounds good.\"}, {\"type\":\"heading\",\"text\":\"Onion messages\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/pull/759>\"}, {\"type\":\"paragraph\",\"text\":\"On onion messages what is the status? We have two implementations that support it that are compatible. I know that since Matt has been reviewing it actively we should wait for more feedback. We can just not touch it anymore because we know that we are compatible but still wait for more feedback?\"}, {\"type\":\"paragraph\",\"text\":\"It is up to you what you want to do. I am not going to have time in the next two weeks to do it. I am hoping I will be able to make it my holiday project and find some time over the end of December. But we have been pretty swamped. I would say don\u2019t spend too much time worrying about waiting for me. I hope to have time to do it at the end of this month but there\u2019s never any promises there.\"}, {\"type\":\"paragraph\",\"text\":\"Whether it is merged or not it doesn\u2019t stop us from continuing our work on offers. We are actively working on offers. It doesn\u2019t change much for us if it is merged or if we just wait a month or two before we merge it.\"}, {\"type\":\"paragraph\",\"text\":\"Your plan is to do the route blinding PR first right because it depends on some of those commits at least? Is that correct?\"}, {\"type\":\"paragraph\",\"text\":\"Yes, exactly.\"}, {\"type\":\"paragraph\",\"text\":\"I am tempted to merge route blinding and onions because we have our interoperability test and we can say they are not going to change. Having changed this multiple times it is not actually that bad to change in practice. What you do is deprecate the old ones, you assign a new message number and you can do anything you want in the new onions. You can support both, it is a bit of a dance but if we were to find some issue\u2026 It doesn\u2019t hurt anyone who doesn\u2019t use it for a start. If we find there is some crypto issue, we should really do it this way instead then we can bump the message number by 2 and do our variation of the scheme there. It is not that bad. I do like the idea of merging it in because that fires the starting gun for people to go \u201cWe should really implement this now or at least look at it\u201d. I vote that we merge those two having passed interop test.\"}, {\"type\":\"paragraph\",\"text\":\"There is just one thing that your comment made me think about. If it is true if we are only using route blinding and onion messages it is really easy to move to a different version where we for example change the internals of route blinding. But if you start using it for payments in invoices then you must specify some kind of versioning for this route blinding scheme? If we want to be able to move to a new one?\"}, {\"type\":\"paragraph\",\"text\":\"Not if we do it in BOLT 12. You\u2019d use a modern message to request the invoice. Then you go \u201cYou\u2019re speaking the modern message so I\u2019ll give you a modern TLV\u201d. You can switch the whole basis across because I\u2019ve done this once already. If they ask to use the old onion message we\u2019ll give them an old TLV, an old invoice. It is not pretty, it is layering violation but it does work. This then beds that down, the next thing to do is the route blinding for payments. That is something we can look at as well. I\u2019m not going to commit to two weeks.\"}, {\"type\":\"paragraph\",\"text\":\"A new guy, lightning-developer started reviewing route blinding and onion messages so we can give him a few days or weeks before we merge. Depending on his feedback we merge these two.\"}, {\"type\":\"paragraph\",\"text\":\"ACK.\"}, {\"type\":\"heading\",\"text\":\"Warning messages\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/pull/834>\"}, {\"type\":\"paragraph\",\"text\":\"Let\u2019s finalize warning messages I guess. There\u2019s not much to say.\"}, {\"type\":\"paragraph\",\"text\":\"We had some argument last week but Rusty has not updated the PR it looks like. We are waiting on that.\"}, {\"type\":\"paragraph\",\"text\":\"You are ok to re-add that Rusty? The `O0` errors.\"}, {\"type\":\"paragraph\",\"text\":\"Yes. I will re-add zeros.\"}, {\"type\":\"paragraph\",\"text\":\"I\u2019ll re-read it again and ACK it. We can finally get this merged since it is already live in 3 implementations.\"}, {\"type\":\"paragraph\",\"text\":\"lnd doesn\u2019t.\"}, {\"type\":\"paragraph\",\"text\":\"LDK, c-lightning and eclair.\"}, {\"type\":\"paragraph\",\"text\":\"We never merged it. I have tested it with Rusty but we haven\u2019t merged the PR.\"}, {\"type\":\"heading\",\"text\":\"Clarify channel_reestablish requirements\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/pull/932>\"}, {\"type\":\"paragraph\",\"text\":\"I created an issue to make it more clear. And I opened issues on lnd and c-lightning related to that.\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/issues/934>\"}, {\"type\":\"paragraph\",\"text\":\"It makes sense but I don\u2019t know what we actually do in practice.\"}, {\"type\":\"paragraph\",\"text\":\"I haven\u2019t tested again. He was pretty sure lnd does automatically close before receiving.. but c-lightning he was not entirely sure. We started to implement a new mode for node operators of big nodes who are actively monitoring their node and don\u2019t want to take any risks. They can configure this new strategy when we detect that we are late and the other guy says we are late, we print a big log in a specific log file and send a notification on Telegram or something to the node operator. Give them an opportunity to fix it before they lose thousands of channels. If they messed up something with TLV or.. We then realized it didn\u2019t make sense to implement it right now because our peers would close anyway regardless of what we did.\"}, {\"type\":\"paragraph\",\"text\":\"It makes sense. I see that Eugene confirms that lnd does close. Is there a plan to fix that? I opened an [issue](https://github.com/lightningnetwork/lnd/issues/6017) on the lnd repo about that.\"}, {\"type\":\"paragraph\",\"text\":\"I saw the issue, no plan currently. But that doesn\u2019t mean it won\u2019t happen.\"}, {\"type\":\"heading\",\"text\":\"Simplified update and PTLCs\"}, {\"type\":\"paragraph\",\"text\":\"<https://github.com/lightning/bolts/pull/867>\"}, {\"type\":\"paragraph\",\"text\":\"One thing I wanted to discuss is the simplified update because it is related to something I [posted](https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-December/003377.html) on the mailing list today. The reason I\u2019m bringing this one up again is I started looking into PTLCs recently and what was the way we could get a minimal version of PTLCs with a minimal set of changes to the existing protocol and transaction format. I discussed it with AJ who agrees that his proposal is a more long term thing. We should start with getting PTLCs on top of the existing structure. But actually there is one roadblock, there is one thing that PTLCs completely change compared to payment secrets. I posted that on the mailing list today and I have a detailed article on it. With a payment secret and preimage, when someone spends a HTLC success or claims a HTLC success you discover the secret by just watching the witness of the script. But with PTLCs it is different. When someone claims a PTLC success you don\u2019t discover anything, the only way to discover it is if you had received an adaptor signature before. That means you have to receive adaptor signatures before you can sign your commitment. Worse than that it also means that when the remote commitment is published you cannot directly claim from it. You have to go through a pre-signed transaction that gave the other guy the opportunity to have an adaptor signature so that they are able to discover the secret when you claim it. We don\u2019t have the easy structure that we had where I only sign things that go to your transactions, you only sign mine and I don\u2019t need to give you signatures for anything from my local commitment. But now we do. I think that the current protocol of commit sig, revoke and ack, commit sig, revoke and ack doesn\u2019t fit that model well. I think it is time to change it. If we change it we should try to find something that is somewhat similar to what we have so it is not too much of a complex change. Something that is compatible probably with an option simplified commitment and that works for both HTLCs and this first version of PTLCs. My mail to the mailing list was a call to action to protocol designers to propose some things without proposing something too complex or too different from what we have.\"}, {\"type\":\"paragraph\",\"text\":\"I haven\u2019t worked on option simplified commitment in a while. An option simplified commitment is literally a subset of what we have now which is nice. It is significantly simpler. It doesn\u2019t win you much in code until you remove the old version of course. You still have to support the whole state machine. One thing about is option simplified commitment that is worth noting, at the moment there are some things that you have to tell your peer to never send you because you don\u2019t ever want to see them in your commitment message because to fail a HTLC you have to go through a whole cycle. If you switch to option simplified commitment it is actually easy to NACK a commitment without changing the state machine. They send you the commitment signed and you go \u201cNo I told you I don\u2019t want that\u201d. They go round again. That means you never have to give any explicit limits to your peer which is really good because that has been a source of all kinds of channel breaking bugs. Your peer thinks they can do something, you think they shouldn\u2019t do it and they do it. The other reason I want option simplified commitment is there is a simple extension to it that we can do later that allows you to NACK a commitment signed. Then you don\u2019t have any restrictions on your peer. You send me whatever you want, I\u2019ll just NACK the ones I don\u2019t like. I\u2019ll instant fail HTLCs for you. I really want option simplified commitment for that because it simplifies the thing further as well. The more I think about it the more I really like this idea. I hadn\u2019t realized the PTLC thing. There\u2019s that and an extra round trip. I guess we will discuss it on the mailing list.\"}, {\"type\":\"paragraph\",\"text\":\"At least one I guess. That\u2019s the hard part. If we didn\u2019t have to have a new pre-signed transaction for the case where the remote commitment is published it would be fine. We\u2019d just send adaptor signatures before our commitment signed but in the other direction. If I want to send you my commitment signed you would just have to send me all your adaptor signatures before and I can do the same. Now there\u2019s also a pre-signed transaction in the remote commit and we have to both share this new signature for that but also a new adaptor signature for that before. That makes it a bit messy. That is why it is a bit hard to find the right way to translate what we have today into something that works for this case and is not a complete mess where people will get confused between what is in my transaction, what is in yours, what\u2019s an adaptor signature. That is some whiteboard design.\"}, {\"type\":\"paragraph\",\"text\":\"I guess we will have to discuss that on the mailing list. I do like option simplified commitment. I think in practice it doesn\u2019t actually slow things down very much because you just end up batching more as things go back.\"}, {\"type\":\"paragraph\",\"text\":\"Especially since what we realized with PTLCs, in a way you have to do some kind of simplified commitment. You cannot really stream in both directions because before sending your commit sig you have to wait for the other guy to send something. That is why I thought about option simplified commitment and making sure that they would work together. I think option simplified commitment, if we have drafted the rest, can be a good first step towards the protocol change for PTLCs.\"}, {\"type\":\"paragraph\",\"text\":\"Yes. The other thing about option simplified commitment is it makes update a lot easier. At the moment the channel update proposal uses this quiescent state where you have to make sure that nobody has got anything in flight. That is always true in option simplified commitment. At the beginning of your turn by definition it is static. That is why the spec is written in the twisted way it is. You have to be quiescent, you are always quiescent at the beginning. Any significant changes are much easier in this model. It is my fault because a certain person encouraged me to write the optimal algorithm in the first place and I should have pushed back and said \u201cNo let\u2019s start with something simple like this\u201d. This was my original scheme by the way. A simplex rather than a duplex protocol but lessons learned, I\u2019ll take that one. Playing with implementations of this is probably useful too.\"}, {\"type\":\"paragraph\",\"text\":\"I think it is important to start thinking right now about how we could do PTLCs and not paint ourselves in a corner with a protocol change that would make it harder to do PTLCs than what we could ideally do. I would really like to have PTLCs in 2022, at least a first version of it.\"}",
    "body": "\nName: Lightning specification call\n\nTopic: Agenda below\n\nLocation: Google Meet\n\nVideo: No video posted online\n\nAgenda: <https://github.com/lightning/bolts/issues/943>\n\nThe conversation has been anonymized by default to protect the identities of the participants. Those who have given permission for their comments to be attributed are attributed. If you were a participant and would like your comments to be attributed please get in touch.\n\n# Add payment metadata to payment request\n\n<https://github.com/lightning/bolts/pull/912>\n\nIn theory you could have an invoice that is so large that it forces you to use a direct payment. But that is a pretty bad theoretical case. We could make some handwavey comment that users should be aware that it has to go in the onion so it should be of limited length. But that is like saying \u201cDon\u2019t do anything stupid\u201d.\n\nI asked on the issue before the meeting, is there any concern with someone who is making a repeated payment who tries to detect the length of the route in use by varying the payment data length over time across various retries?\n\nHow would they be able to do that, because of the payment secret thing?\n\nAs long as you know when a payment failed. If you have the ability to send someone ten invoices and have them try them in order and detect which ones fail then you would have the ability to figure out the length of the path they are using, at least currently.\n\nYou mean the recipient is de-anonymizing the sender? Not an intermediate node?\n\nBasically, yes.\n\nMaybe an intermediate option is to limit it to a large number just to prevent that.\n\nThat was basically what I was suggesting, limit it to something. I don\u2019t know if you need 700, limit it to 256 or something.\n\n640 is traditional.\n\n420.\n\nI would make it longer than 28 because that seems really small. There might be some applications that are excluded by that. I think it is already quite exotic what you described. You are able to supply multiple invoices that are all attempted and you are going to derive something out of it about the distance. I would pick a large value.\n\nNote that any amount we limit it to is basically plus 32 because you can always use the payment secret as well.\n\nThere are other fields that are always there. What\u2019s the minimum for a direct payment?\n\nThat attack already exists because I can hand you a fake route hint of arbitrary length that you have to use to make the payment, therefore leaving less room in the onion.\n\nThis is true.\n\nShall I add a comment to the PR that you have to be aware that it needs to fit in the onion including the route?\n\nYeah.\n\nI did also comment on the PR. I think if we do that then we should at least also say that a payer must not limit the length except when they create the onion ie if someone knows they are well connected node they can just use a larger value. The payer must not apply any concrete limits.\n\nTo prevent the opposite thing, people limiting more than necessary.\n\nYes, exactly.\n\nLet\u2019s add that, let\u2019s implement it and let\u2019s do compatibility testing again.\n\n# Advertize compression algorithms in init\n\n<https://github.com/lightning/bolts/pull/825>\n\nLet\u2019s move onto the next one, the compression algorithm support thing. It is an old one, it was removing the dependency on zlib for implementations that did not want to support it. At the same time make it flexible enough to add new encodings or new compression algorithms if we ever need to. Make sure that this is also something that is more general than gossip queries if we ever need to. We can do it for free even though I\u2019m sure we\u2019ll ever use it. Basically you send a bitfield in a TLV in your init saying \u201cHere are the compression algorithms I support\u201d. People for gossip queries only use the things that both you and I support. We have tested with Matt on testnet and it worked great with LDK and eclair. It is not very hard to implement.\n\nIt is so much easier to implement zlib than it is to actually implement this.\n\nIt adds a bit of complexity. It is still one feature. It is not very hard.\n\nI think what happens in the long term, we will end up using Minisketch for gossip I\u2019m pretty sure. This becomes less important. I am happy to take your advice. If having implemented it if you are happy with the complexity of it sure.\n\nI find it simple but it provides extension points that I think we don\u2019t really need. But maybe we never use them so it is fine. I agree that the next changes we will make to gossip queries or to gossiping overall will be mostly a complete rewrite using something like Minisketch. We will change everything so we will throw this away. But I can understand that if LDK needs something to start with\u2026\n\nThere are a lot of nodes on the network that already use non-zlib. In general it isn\u2019t really an issue if you connect to enough nodes to get one that will send you uncompressed data. Given that I am not sure which nodes it is. Does lnd send uncompressed data by default or something?\n\nIt did for a while. We do a heuristic where we look at the compressed length and we go \u201cIt is not worth compressing\u201d which is probably a waste of time. We\u2019ve compressed it already, we should probably just use it. We do go \u201cThat didn\u2019t win so we won\u2019t bother\u201d. If you ask for enough you\u2019ll get compressed data.\n\nIt seems like less complexity than that. My point there was that it seems there are a number of nodes on the network that need this basically.\n\nI think everyone has implemented decompression, I think some people haven\u2019t implemented compression so that is what you are seeing. Implementing decompression is the easier part. The compression involves heuristics, you\u2019ve got to fit it in a message and you\u2019re like \u201cI don\u2019t know how much it is going to compress until I compress it\u201d and stuff like that.\n\nInterestingly we do already have these hooks. We already have the compression flag stuff. At least on the LDK end I just reuse those enums for compression because they happen to line up, at least currently.\n\nBasically I think the state of this PR is we know we can do it, it is not too hard but do we really want to do it? Do we need it?\n\nIt is going to be quite a while before we rewrite stuff and not everyone wants the zlib dependency because zlib has not had the best history of ensuring its decompression library is safe.\n\nIt is just you, everyone else is happy with it, everyone else is stuck with it is the truth. zlib has a bad name but those are in the past. It has been pretty thoroughly vetted now. zlib is a lot smaller. The main issue with zlib was the whole explosive compression thing where you could really cram a lot of data in if people made assumptions about how much they could get out. Which is why the spec went through and figured out the maximum realistic size. If you get more than that you just throw it away. I don\u2019t know. Shying away from zlib because it has bugs, if you can\u2019t trust zlib who can you trust?\n\nThere is just not enough data in the network graph right now to care and so it is very easy to say \u201cWe can avoid bugs by simply not compressing because we don\u2019t care and eventually we will move to something that is substantially more proficient than any of this anyway\u201d.\n\nI do buy that argument.\n\nBut then if we do that it is maybe not worth integrating this PR and instead phasing out zlib.\n\nYou mean drop zlib entirely? We could do that. I have a feeling that is not going to fly mostly with y\u2019all on the mobile side. I know mobile using this stuff is important assuming you are doing graph sync on mobile.\n\nWe don\u2019t care for Phoenix because there\u2019s no graph sync. I don\u2019t think eclair mobile even has the zlib part. Maybe but I\u2019m not sure. I will ask Breez to see if they care and if they extensively use zlib.\n\nGoing back to first principles I\u2019d be quite happy to just drop zlib. I would want to check some numbers but that is simple. I could do that commit. And it is perfectly backwards compatible. Stop compressing, keep decompressing and then at some point turn off decompression.\n\nI\u2019ll try to get in touch with a few wallets. See if they are using it, if they even know if they are using it or not. And if they can measure the difference between the two and see if they really need it. Maybe they\u2019ll say that the fact that it uses more battery is more of an issue for them than the fact that it uses more bandwidth. I don\u2019t even know if it is something that is important for them.\n\nYou are just not going to download the graph fast enough on mobile to want to do that anyway. At least not from a peer-to-peer node.\n\nEugene is saying that lnd doesn\u2019t even have a config option for zlib by default. Unless Breez forked it they couldn\u2019t be using zlib at all. I\u2019ll check with the Breez team and if they are not using it I guess we can go and start deprecating it. At least open a PR and tell the mailing list and see if anyone says that they absolutely want to keep it.\n\n# Dynamic DNS support in gossip messages\n\n<https://github.com/lightning/bolts/pull/911>\n<https://github.com/lightning/bolts/pull/917>\n\nI guess the second one we don\u2019t even need to talk about it because nothing has changed since last time. There is an eclair implementation and there is an ongoing c-lightning one but it is not ready yet. But the DNS one I haven\u2019t done it on eclair at all. I don\u2019t know if anyone else has worked on it?\n\n911 we have merged at least as an experimental option. DNS support we have. We can put DNS fields in advertizements, it seems to work pretty much as you\u2019d expect. It is a fairly straightforward change. 911 we have implemented, we should probably get someone else to implement it as well. There is a draft PR for 917, the init message change. Without Michael (Schmoock) here I don\u2019t know what the status is but I can look at it.\n\nI think it is not filtering local addresses yet. But apart from that he has the TLV and I tested that the TLV decodes fine on the eclair side. But he doesn\u2019t yet filter local addresses.\n\nThat\u2019s weird because we have that code already. That\u2019s how we decide whether or not by default we advertize our own address. He probably just hasn\u2019t hooked it up.\n\nI think we can discuss these two when there is momentum in the implementations.\n\nThey are completely independent. I guess it would be nice if somebody implemented DNS, at least the lookup side.\n\nOne question on DNS. Punycode or UTF-8? It should be specified.\n\nInterestingly in the offers spec we add a UTF-8 type to the fundamental types which is basically a byte string. It gives you a nice hint that something should be in an array of UTF-8. It might be useful to pull that in. It should be specified.\n\nI vaguely prefer that this be ASCII and Punycode, DNS anyway. Things that do DNS tend to do the reverse resolution fairly well. I\u2019m ok being overruled. It is just easier to walk a string and check that it is ASCII than it is to walk a string and check that it is UTF-8 with no control characters.\n\nWhat\u2019s the worst that can happen with UTF-8? It is ourself announcing and as far as I can see the only thing that we could get tricked into is trying to resolve this kind of stuff. It is not like the human is going to read it and get tricked by characters that look alike.\n\nUltimately when you do the resolution you convert it to ASCII and Punycode anyway. If you are looking to do that you might as well just do that upfront. Most of the DNS applications that I\u2019ve seen will do that reverse resolution for you if you want it.\n\nIs that true?\n\nI know at least in some web browsers if you type in the Punycode it will show it to you as the UTF-8 as long as the TLD is one of the internationalized ones. At least this used to be true.\n\nI think there are some domains you won\u2019t be able to reach if you can\u2019t do Unicode but I could be wrong on that.\n\nNo everything gets converted to ASCII. You take the UTF-8, you convert it to ASCII with this weird prefix, accent dash dash or something like that, I forget what the actual prefix is. You encode the UTF-8 as ASCII and then you do the resolution.\n\nI am not a Punycode expert but that means that Punycode by definition is at least as long as UTF-8 right? Would that be an argument? It is gossip data that has to be replicated on each individual node so any byte we can save there might be a massive win for us. If we have to have some sort of tiebreaker.\n\nIt is only an extra byte or two. Obviously I prefer ASCII for everything because it is easier than trying to think about attack scenarios but if someone feels very strongly about byte length I\u2019m ok with that too.\n\nI guess I would prefer ASCII as well because it is just simpler. People can just find an ASCII hostname, there is no good reason to have something that is not ASCII for Lightning nodes.\n\nIf people insist on having weird hostnames they will shoot themselves in the foot by them paying the cost to advertize that. Probably going to stick with ASCII myself.\n\nEveryone pays the price because it is gossiped. To be honest I just looked up Punycode, I had completely forgotten that existed. I am a little bit horrified. I remember before UTF-8 ruled the world. A certain amount of PTSD. I would say yes, it is ASCII due to DNS limitations, that is fine.\n\nAlright, I will comment on the PR.\n\nAnd make sure you put a link to the Wikipedia Punycode article so people can share the horror if they haven\u2019t been exposed before.\n\nAnd encode the link in Punycode itself.\n\nYeah, do that.\n\nTo one of internationalized versions.\n\n# BLIPs\n\n<https://github.com/lightning/blips>\n<https://github.com/lightning/blips/pull/4>\n\nI am ambivalent about whether they should go in together or go in separate. Whichever allows us to get this stuff merged before the year ends is what I\u2019m in favor of.\n\nMaybe separate is a good idea. The one I opened will be updated very often. Every time someone adds a BLIP they will have to open this table of things that they want to reserve. Whereas they should not open things about the meta process of adding a BLIP.\n\nAgreed, I think that makes sense.\n\nKeeping the two separate I think makes sense. I haven\u2019t read the latest version of BLIP 1 since it has moved to the new repo. I need to do that right now.\n\nI listed in the initial commit the changes that I made. One was per Matt\u2019s feedback out of this specific mandatory universality section requesting the proposals discuss why the given features are not intended to be universal. That\u2019s a must now. Add a little bit more detail around what ranges of feature bits and TLVs belong here. Between 100 and 1000 for experimentation on feature bits and then TLVs above 65, 536. Then a couple of links and stuff. Otherwise I think it is largely the same that we were close to having consensus on in the previous repo.\n\nI guess it sounds like a good start to me. I think we will probably change some of those as we go, as we learn how people write BLIPs and what are the pain points. I think we should start with something as small and simple as we can. And then make it evolve as we learn more about the process. That looks good enough.\n\nThis looks good to me. I\u2019m sure we are going to have some level of friction over the status field. The BIP process uses them for a little bit of pseudo inside baseball sometimes. I have a feeling that we are going to want to revisit these as we go but that is ok. I don\u2019t think we need to figure that out now.\n\nI think in general having this as a starting point and continually iterating on it as people start using the process makes a lot of sense.\n\nI agree. I will do a real review this week. It is a conceptual ACK from me at least.\n\nI will do the same for yours. For PR 4.\n\n# Route blinding\n\n<https://github.com/lightning/bolts/pull/765>\n\nThere has been a new review on route blinding. There has been a complete compatibility test between c-lightning and eclair on the latest version of onion messages. Did anyone have time to look at the route blinding one? My main feedback from the last meeting, I was asking what parts I should remove to get a first version in. Mostly probably the parts about the payments and I should keep only the parts that are library cryptographic utilities that are being used by onion messages. Should I keep it in the proposal document where it is not specified yet and we can iterate on it? I updated the test vectors as well and I think they should be easier for you to work with and build upon for onion messages.\n\nGreat. We have had route blinding for payments for well over a year as an experimental option. Basically unused except for the test code because there was no good way to specify it. When we tried to revise it to the latest thing we hit those issues that I commented on the PR. But it works really well for the onion message as a base and we have interop testing so I am tempted to leave it there in the proposal document but just not in the spec.\n\nThat sounds good to me. I didn\u2019t put anything in the BOLTs related to messages, I left it in the proposal. I think the only important parts to discuss are the potential unblinding attacks that you would do by probing fees, probing CLTV expiry delta. It is basically things that we recommend implementations do like use the same CLTV expiry delta across the whole route. Use something that is different from the public values but maybe higher, same for the fees. I am not sure how to best convey those in the spec.\n\nWhile I can specify that you should use the same value across you can always ignore those values and choose to probe the differences. The ultimate answer is you put it in the encrypted TLV, you put \u201cBy the way please enforce this value\u201d so they can\u2019t play with it.\n\nThat makes it bigger.\n\nPerfect is the enemy of the good, yeah. That is another thing that can potentially be added later. I think for now a graph is relatively homogenous anyway so you wouldn\u2019t get all that much data. Although you would get some doing those games. We have a way of fixing it later if we want.\n\nWhat would you tell an intermediate node that is inside the blinded route when they receive a payment that they should forward and the fee is not enough or the CLTV is not enough? What error should they answer?\n\nThey have to fail. We have an error that we return from anywhere in the blinded tunnel, in that TLV we return the same error. In fact our implementation, if it was blinded we always reply with the same error. You\u2019ll actually see it from the entrance to that TLV scheme and you\u2019ll never see an error from the middle. That\u2019s partially to protect against this attack. Although it is not perfect because you can still do success or failure tests. You get one bit of information out but you can\u2019t tell exactly where it comes from. I\u2019ll have to look up the code to see what we do but we have a specific error, I think we added an error for it. It has been a while, I\u2019ll have to look.\n\nFrom a honest sender\u2019s point of view if you get such an error that tells you something wrong happened inside the blinded route, maybe it is fees, maybe it is CLTV expiry delta, should you just retry by raising everything for every blinded hop so that you have a chance of success? I think you should because there is a chance that your payment will go through. This is why it makes it better compared to rendezvous, you can retry, still use that blinded route and adapt to dynamic fees changing.\n\nThat is really hard to know. Your chances of success at that stage have surely dropped significantly. It is really up to the person handing you the route to have done that padding if any for you. If they haven\u2019t maybe you could retry but you don\u2019t actually know what is going on. It could be that there\u2019s temporary disconnect in the route, it is not working anymore. That\u2019s possibly the more likely case. It just won\u2019t work. In which case you are kind of out of luck at that point. Unless they hand you multiple encrypted routes which is allowed in the spec but we haven\u2019t implemented it.\n\nI think that makes sense from the recipient\u2019s point of view. Especially if you have multiple entry points. Just give most of them in the invoice. The idea is the same way you do route hints, you\u2019d now use these. You can hand out multiple. The case where you don\u2019t allow it\u2026 For payments it is a little bit different. For onion messages you only give a single reply path but the idea is you will resend if it doesn\u2019t work and maybe try a different reply path.\n\nEven from a payment\u2019s point of view as a recipient if you are not well connected at all, you just have one entry point, since it is blinded you can still make it look like you have many entry points and provide many fake route hints. I think it is a good idea to have this option.\n\nI agree. That\u2019s for payments. We\u2019d need to test that. But for onion messages this route blinding works really well.\n\nI saw that Tim made a comment about typo. Do you know if he has some time available to review the crypto? I discussed it with Jonas Nick in El Salvador. He said it could be on his list because he didn\u2019t realize it was a requirement for offers. They have so much to do already, I\u2019m not sure if they will find the time to take a look at it.\n\nI will beg and see what happens.\n\nSounds good.\n\n# Onion messages\n\n<https://github.com/lightning/bolts/pull/759>\n\nOn onion messages what is the status? We have two implementations that support it that are compatible. I know that since Matt has been reviewing it actively we should wait for more feedback. We can just not touch it anymore because we know that we are compatible but still wait for more feedback?\n\nIt is up to you what you want to do. I am not going to have time in the next two weeks to do it. I am hoping I will be able to make it my holiday project and find some time over the end of December. But we have been pretty swamped. I would say don\u2019t spend too much time worrying about waiting for me. I hope to have time to do it at the end of this month but there\u2019s never any promises there.\n\nWhether it is merged or not it doesn\u2019t stop us from continuing our work on offers. We are actively working on offers. It doesn\u2019t change much for us if it is merged or if we just wait a month or two before we merge it.\n\nYour plan is to do the route blinding PR first right because it depends on some of those commits at least? Is that correct?\n\nYes, exactly.\n\nI am tempted to merge route blinding and onions because we have our interoperability test and we can say they are not going to change. Having changed this multiple times it is not actually that bad to change in practice. What you do is deprecate the old ones, you assign a new message number and you can do anything you want in the new onions. You can support both, it is a bit of a dance but if we were to find some issue\u2026 It doesn\u2019t hurt anyone who doesn\u2019t use it for a start. If we find there is some crypto issue, we should really do it this way instead then we can bump the message number by 2 and do our variation of the scheme there. It is not that bad. I do like the idea of merging it in because that fires the starting gun for people to go \u201cWe should really implement this now or at least look at it\u201d. I vote that we merge those two having passed interop test.\n\nThere is just one thing that your comment made me think about. If it is true if we are only using route blinding and onion messages it is really easy to move to a different version where we for example change the internals of route blinding. But if you start using it for payments in invoices then you must specify some kind of versioning for this route blinding scheme? If we want to be able to move to a new one?\n\nNot if we do it in BOLT 12. You\u2019d use a modern message to request the invoice. Then you go \u201cYou\u2019re speaking the modern message so I\u2019ll give you a modern TLV\u201d. You can switch the whole basis across because I\u2019ve done this once already. If they ask to use the old onion message we\u2019ll give them an old TLV, an old invoice. It is not pretty, it is layering violation but it does work. This then beds that down, the next thing to do is the route blinding for payments. That is something we can look at as well. I\u2019m not going to commit to two weeks.\n\nA new guy, lightning-developer started reviewing route blinding and onion messages so we can give him a few days or weeks before we merge. Depending on his feedback we merge these two.\n\nACK.\n\n# Warning messages\n\n<https://github.com/lightning/bolts/pull/834>\n\nLet\u2019s finalize warning messages I guess. There\u2019s not much to say.\n\nWe had some argument last week but Rusty has not updated the PR it looks like. We are waiting on that.\n\nYou are ok to re-add that Rusty? The `O0` errors.\n\nYes. I will re-add zeros.\n\nI\u2019ll re-read it again and ACK it. We can finally get this merged since it is already live in 3 implementations.\n\nlnd doesn\u2019t.\n\nLDK, c-lightning and eclair.\n\nWe never merged it. I have tested it with Rusty but we haven\u2019t merged the PR.\n\n# Clarify channel_reestablish requirements\n\n<https://github.com/lightning/bolts/pull/932>\n\nI created an issue to make it more clear. And I opened issues on lnd and c-lightning related to that.\n\n<https://github.com/lightning/bolts/issues/934>\n\nIt makes sense but I don\u2019t know what we actually do in practice.\n\nI haven\u2019t tested again. He was pretty sure lnd does automatically close before receiving.. but c-lightning he was not entirely sure. We started to implement a new mode for node operators of big nodes who are actively monitoring their node and don\u2019t want to take any risks. They can configure this new strategy when we detect that we are late and the other guy says we are late, we print a big log in a specific log file and send a notification on Telegram or something to the node operator. Give them an opportunity to fix it before they lose thousands of channels. If they messed up something with TLV or.. We then realized it didn\u2019t make sense to implement it right now because our peers would close anyway regardless of what we did.\n\nIt makes sense. I see that Eugene confirms that lnd does close. Is there a plan to fix that? I opened an [issue](https://github.com/lightningnetwork/lnd/issues/6017) on the lnd repo about that.\n\nI saw the issue, no plan currently. But that doesn\u2019t mean it won\u2019t happen.\n\n# Simplified update and PTLCs\n\n<https://github.com/lightning/bolts/pull/867>\n\nOne thing I wanted to discuss is the simplified update because it is related to something I [posted](https://lists.linuxfoundation.org/pipermail/lightning-dev/2021-December/003377.html) on the mailing list today. The reason I\u2019m bringing this one up again is I started looking into PTLCs recently and what was the way we could get a minimal version of PTLCs with a minimal set of changes to the existing protocol and transaction format. I discussed it with AJ who agrees that his proposal is a more long term thing. We should start with getting PTLCs on top of the existing structure. But actually there is one roadblock, there is one thing that PTLCs completely change compared to payment secrets. I posted that on the mailing list today and I have a detailed article on it. With a payment secret and preimage, when someone spends a HTLC success or claims a HTLC success you discover the secret by just watching the witness of the script. But with PTLCs it is different. When someone claims a PTLC success you don\u2019t discover anything, the only way to discover it is if you had received an adaptor signature before. That means you have to receive adaptor signatures before you can sign your commitment. Worse than that it also means that when the remote commitment is published you cannot directly claim from it. You have to go through a pre-signed transaction that gave the other guy the opportunity to have an adaptor signature so that they are able to discover the secret when you claim it. We don\u2019t have the easy structure that we had where I only sign things that go to your transactions, you only sign mine and I don\u2019t need to give you signatures for anything from my local commitment. But now we do. I think that the current protocol of commit sig, revoke and ack, commit sig, revoke and ack doesn\u2019t fit that model well. I think it is time to change it. If we change it we should try to find something that is somewhat similar to what we have so it is not too much of a complex change. Something that is compatible probably with an option simplified commitment and that works for both HTLCs and this first version of PTLCs. My mail to the mailing list was a call to action to protocol designers to propose some things without proposing something too complex or too different from what we have.\n\nI haven\u2019t worked on option simplified commitment in a while. An option simplified commitment is literally a subset of what we have now which is nice. It is significantly simpler. It doesn\u2019t win you much in code until you remove the old version of course. You still have to support the whole state machine. One thing about is option simplified commitment that is worth noting, at the moment there are some things that you have to tell your peer to never send you because you don\u2019t ever want to see them in your commitment message because to fail a HTLC you have to go through a whole cycle. If you switch to option simplified commitment it is actually easy to NACK a commitment without changing the state machine. They send you the commitment signed and you go \u201cNo I told you I don\u2019t want that\u201d. They go round again. That means you never have to give any explicit limits to your peer which is really good because that has been a source of all kinds of channel breaking bugs. Your peer thinks they can do something, you think they shouldn\u2019t do it and they do it. The other reason I want option simplified commitment is there is a simple extension to it that we can do later that allows you to NACK a commitment signed. Then you don\u2019t have any restrictions on your peer. You send me whatever you want, I\u2019ll just NACK the ones I don\u2019t like. I\u2019ll instant fail HTLCs for you. I really want option simplified commitment for that because it simplifies the thing further as well. The more I think about it the more I really like this idea. I hadn\u2019t realized the PTLC thing. There\u2019s that and an extra round trip. I guess we will discuss it on the mailing list.\n\nAt least one I guess. That\u2019s the hard part. If we didn\u2019t have to have a new pre-signed transaction for the case where the remote commitment is published it would be fine. We\u2019d just send adaptor signatures before our commitment signed but in the other direction. If I want to send you my commitment signed you would just have to send me all your adaptor signatures before and I can do the same. Now there\u2019s also a pre-signed transaction in the remote commit and we have to both share this new signature for that but also a new adaptor signature for that before. That makes it a bit messy. That is why it is a bit hard to find the right way to translate what we have today into something that works for this case and is not a complete mess where people will get confused between what is in my transaction, what is in yours, what\u2019s an adaptor signature. That is some whiteboard design.\n\nI guess we will have to discuss that on the mailing list. I do like option simplified commitment. I think in practice it doesn\u2019t actually slow things down very much because you just end up batching more as things go back.\n\nEspecially since what we realized with PTLCs, in a way you have to do some kind of simplified commitment. You cannot really stream in both directions because before sending your commit sig you have to wait for the other guy to send something. That is why I thought about option simplified commitment and making sure that they would work together. I think option simplified commitment, if we have drafted the rest, can be a good first step towards the protocol change for PTLCs.\n\nYes. The other thing about option simplified commitment is it makes update a lot easier. At the moment the channel update proposal uses this quiescent state where you have to make sure that nobody has got anything in flight. That is always true in option simplified commitment. At the beginning of your turn by definition it is static. That is why the spec is written in the twisted way it is. You have to be quiescent, you are always quiescent at the beginning. Any significant changes are much easier in this model. It is my fault because a certain person encouraged me to write the optimal algorithm in the first place and I should have pushed back and said \u201cNo let\u2019s start with something simple like this\u201d. This was my original scheme by the way. A simplex rather than a duplex protocol but lessons learned, I\u2019ll take that one. Playing with implementations of this is probably useful too.\n\nI think it is important to start thinking right now about how we could do PTLCs and not paint ourselves in a corner with a protocol change that would make it harder to do PTLCs than what we could ideally do. I would really like to have PTLCs in 2022, at least a first version of it.\n\n\n",
    "body_type": "markdown",
    "created_at": "2021-12-06T00:00:00.000Z",
    "domain": "https://btctranscripts.com/",
    "url": "https://btctranscripts.com/lightning-specification/2021-12-06-specification-call",
    "categories": [
        "meetup"
    ],
    "tags": [
        "lightning"
    ],
    "indexed_at": "2024-03-21T16:33:36.664Z",
    "transcript_by": "Michael Folkson",
    "summary": "In the realm of digital finance and cryptocurrency, the Lightning Network stands as a beacon for scaling solutions. A recent podcast delved deep into the ongoing developments and challenges faced by this innovative protocol. At the forefront of the conversation was an exploration of encoding preferences for Lightning nodes. The consensus among participants favored ASCII over other formats due to its simplicity and compatibility with existing DNS limitations. This preference underscores a pragmatic approach to the network's architectural design choices, favoring straightforwardness and broad applicability.\n\nThe management of Bitcoin Lightning Improvement Proposals (BLIPs) was another significant topic of discussion. Participants examined the procedural nuances of proposing, reviewing, and merging improvements into the Lightning network protocol. A notable suggestion was the separation of technical specification proposals from those addressing the meta-process of adding a BLIP. This bifurcation aims to streamline the evaluation process, ensuring clarity, efficiency, and transparency in how proposals are handled and why certain features may not be universally designed.\n\nPrivacy and security within the network were highlighted through discussions on route blinding. This feature, aimed at concealing a payment's complete route from intermediate nodes, exemplifies the ongoing efforts to refine the network's capabilities. Despite the complexities associated with implementing route blinding for payments, its successful application in onion messages\u2014used for encrypted data transmission over the network\u2014demonstrates a balancing act between striving for perfection and deploying functional versions that can be iteratively improved.\n\nMoreover, the conversation touched upon the integration of warning messages and the status of onion messages, both central to enhancing the functionality and user experience of the Lightning Network. Onion messages facilitate anonymous and secure communication between nodes, while warning messages promise to augment this infrastructure. These discussions emphasized the importance of continuous testing, feedback, and iteration to meet the evolving needs and security standards of the network.\n\nA pressing issue addressed was the stagnation of a critical Pull Request (PR), awaiting resolution on the inclusion of `O0` errors. The necessity of a new mode for large node operators was discussed, aiming to prevent automatic channel closures and potential loss of channels due to minor issues. This strategy involves significant event logging and real-time operator alerts for swift issue resolution.\n\nThe potential of Simplified Update and Point Time Locked Contracts (PTLCs) was explored as a means to enhance the network's operations. PTLCs represent a paradigm shift from Hashed Time-Locked Contracts (HTLCs) by not revealing secrets upon transaction completion, thus necessitating a revised protocol to accommodate these changes effectively. The benefits of an option simplified commitment model were discussed, highlighting its flexibility and reliability in updating channel states and managing commitments.\n\nThis comprehensive overview of the podcast reveals the collaborative and dynamic nature of open-source development in advancing the Lightning Network. Through discussions on encoding preferences, the BLIP process, privacy features, and operational enhancements, the conversation sheds light on the complex yet rewarding journey of building scalable, secure, and user-friendly protocols for decentralized micropayments."
}