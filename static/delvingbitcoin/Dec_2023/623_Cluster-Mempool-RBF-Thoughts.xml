<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>1</id>
  <title>Cluster Mempool RBF Thoughts</title>
  <updated>2023-12-29T19:01:29.049944+00:00</updated>
  <author>
    <name>ajtowns 2023-12-05 15:56:26.733000+00:00</name>
  </author>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <entry>
    <id>1</id>
    <title>Cluster Mempool RBF Thoughts</title>
    <updated>2023-12-29T19:01:29.049944+00:00</updated>
    <link href="https://delvingbitcoin.org/t/cluster-mempool-rbf-thoughts/156/10" rel="alternate"/>
    <summary>When considering the potential for a transaction to conflict with 100 in-mempool transactions and allowing each cluster to contain up to 100 transactions, we face the possibility of having to iterate through 10,000 chunks. This number raises concerns about performance, suggesting that such an operation could be slow. However, when evaluating the computational complexity of operating on two sets of 10,000 pairs of `int64_t`, an $O(n)$ complexity appears manageable. The process might require an additional log(n) factor to efficiently combine the clusters.

In order to optimize this process, it's advisable to devise a clever method for generating the new set of clusters. This should ideally be accomplished by using the original set along with the conflicted and new transactions as a base, rather than constructing the new set from scratch. Utilizing the existing information to influence the generation of new clusters can lead to a more efficient update process and mitigate the potential slowdown associated with handling a large number of transactions and clusters.</summary>
    <published>2023-12-05T15:56:26.733000+00:00</published>
  </entry>
</feed>
